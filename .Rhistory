colnames(dat.cor) <- colnames(dat.pvalue)
#pvalue.index <- apply(dat.pvalue, 2, function(x) any(x < 0.01))
#pvalue.index2 <- apply(dat.pvalue, 1, function(x) any(x < 0.01))
#dat.cor.cle <- dat.cor[pvalue.index2, pvalue.index]
dat.cor.cle <- dat.cor
colnames(dat.cor.cle) <- gsub("_p.value", "", colnames(dat.cor.cle))
#dat.pva.cle <- dat.pvalue[pvalue.index2, pvalue.index]
dat.pva.cle <- dat.pvalue2
num<- matrix(NA,nrow = nrow(dat.pva.cle), ncol = ncol(dat.pva.cle))
for(i in 1:ncol(dat.pva.cle)){
num[,i] <- mapply(trans2, dat.pva.cle[,i])
}
#colt<-c("#4C38CB","#9191C8","#DADAEC","#F0C1C1","#E28383","#D44545","#CD2626")
colt3<-c("#164a83","#2565a3","#79b3d5","#bdd6e6","#f3f9fc", "#fff6f1","#df896b","#d06651","#bd433e","#b31f2c")
pheatmap(t(dat.cor.cle),
treeheight_row=43,
treeheight_col=23,
cellwidth=20,
cellheight=8,
cluster_cols=T,
cluster_rows=T,
fontsize_row=8,
fontsize_col=13,
show_colnames=T,
display_numbers=t(num),
color =colt3,
#breaks=seq(-0.6,0.6,0.2),
#legend_breaks = c(-0.6,-0.3, 0, 0.3,0.6),
#legend_labels = c("-0.6","-0.3","0", "0.3", "0.6"),
number_color = "white")
}
dim(species.clean2)
library(rmeta)
library(rmeta)
pip.dat <- readxl::read_excel("../../result/00.data/bjt2d.REPORT.xlsx",col_names = T)
dat.reads.num <- pip.dat[,c("raw.reads","abun_size")]
raw.less20m <- sum(pip.dat$raw.reads<20000000)
abun.less20m <- sum(pip.dat$abun_size<20000000)
dat.reads.num2 <- reshape2::melt(dat.reads.num)
ggplot(dat.reads.num2,aes(value/1e6,color=variable))+
geom_line(stat = "density",size=0.8)+
geom_vline(xintercept = 20,size=1,linetype="dashed")+
scale_x_continuous(breaks = c(0,20,50,100,150))+
theme_classic()+
#mytheme+
xlab("reads number(M paired)")
dat.pip.qc <- pip.dat[,ncol(pip.dat)-c(5,3:0)]
dat.pip.qc2 <- dat.pip.qc[order(dat.pip.qc$mapped.reads),]
nm.order <- rownames(dat.pip.qc2)
dat.pip.qc2$sample <- rownames(dat.pip.qc2)
dat.pip.qc2 <- reshape2::melt(dat.pip.qc2,id.vars = c("coverage","sample"),
variable.name = "stage")
dat.pip.qc2$stage <- factor(dat.pip.qc2$stage,
levels = c("lowQ.reads","host.reads","unmapped.reads","mapped.reads"))
dat.pip.qc2$sample <- factor(dat.pip.qc2$sample,levels = nm.order)
dat.pip.qc2$value <- dat.pip.qc2$value/1e6
ggplot(dat.pip.qc2,aes(sample,value,fill=stage))+
geom_bar(stat = "identity")+
geom_hline(yintercept = 20)+
theme_classic()+
#mytheme+
theme(axis.text.x = element_blank(),
# legend.position = c(0.01,0.99),
#legend.justification = c(0,1),
axis.ticks = element_blank())+
xlab("")+ylab("reads number (M)")+
scale_y_continuous(expand = c(0,0),breaks = c(0,20,40,80,120,160))
library(dunn.test)
a <- c(1:6)
b <- c(7:12)
wilcox.test(a, b)
wilcox.test(a, b)$p.value
wilcox.test(a, b)$p.value*15
?p.adjust
sample(seq(0.001,0.01, 0.01))
sample(seq(0.001,0.01, 0.01), 10)
sample(seq(0.001,0.01, 0.001), 10)
sample(seq(0.001,0.01, 0.001), 10) -> pvalue
p.adjust(pvalue, method = "BH")
p.adjust(pvalue, method = "fdr")
install.packages("rsq")
library(rsq)
data(hcrabs)
attach(hcrabs)
y <- ifelse(num.satellites>0,1,0)
bnfit <- glm(y~color+spine+width+weight,family=binomial)
rsq(bnfit)
rsq(bnfit,adj=TRUE)
bnfit
data(hcrabs)
attach(hcrabs)
y <- ifelse(num.satellites>0,1,0)
bnfit <- glm(y~color+spine+width+weight,family=binomial)
rsq.partial(bnfit)
glm(color~spine+width+weight)
color
spine
width
a <- lm(width~spine+color+weight)
b <- lm(width~spine+weight)
summary(a)
str(summary(a))
summary(a)$adj.r.squared
summary(b)$adj.r.squared
b <- lm(width~spine+color)
summary(b)$adj.r.squared
c <- glm(width~spine+color+weight)
rsq.partial(c)
summary(b)$r.squared
summary(a)$r.squared
rsq.partial
?rsq.partial
lmmode
summary(lmmode)
lmmode$residuals
lmmode$df
lmmode
example(glm)
summary(anorex.1)
anorex.1$df.residual
lmmode
glm(formula = valiPhe ~ risk)
glm(formula = valiPhe ~ risk) -> a
a$df.residual
length(risk)
lmmode$df
lmmode$df[2]
lmmode$r.squared
library(rmeta)
library(rmeta)
c("a", "b", "c")
c("a", "b", "c") -> a
c(TRUE, FALSE, TRUE) -> b
b[a]
a[b]
ifelse(b,a,"")
?xlab
library(coin)
c("a", "b", "c")
paste0(rep(c("a", "b"),3), c("ratio", "mean", "median"))
paste0(rep(c("a", "b"),each=3), c("ratio", "mean", "median"))
comparePair <- function(data, config, group, ...){
# wilcox test on multigroup
nlevle <- levels(config[, group])
if(length(nlevle) >2 ){
combng <- combn(nlevle, 2)
out <- list()
for(i in 1:ncol(combng)){
subconfig <- config[config[,group] %in% combng[,i], ]
out[i] <- mywilcox_2t(data, subconfig, ...)
}
return(out)
}else{
res <- mywilcox_2t(data, config, ...)
return(out)
}
}
mywilcox_2t <- function(datamatrix, configdata, Time2, ratio="zero",...){
# to sort the data
config <- matchID(configdata, ...)
data <- datamatrix[rownames(config), ,drop=F]
# to analysis
out <- matrix(NA, nrow = ncol(data), ncol = 9)
config <- as.factor(config[, Time2])
nlevels = levels(config)
for(i in 1:ncol(data)){
tmp <- as.numeric(data[,i])
g1 <- tmp[config == nlevels[1]]
g2 <- tmp[config == nlevels[2]]
wilcox_sign <- pvalue(wilcoxsign_test(g1~g2))
effect <- wilcoxonPairedR(x <- tmp, g <- config)
if(ratio=="zero"){
or <- tapply(tmp, config, function(x){sum(x!=0, na.rm=T)/length(x)})
}else{
or <- tapply(tmp, config, function(x){sum(!is.na(x))/length(x)})
}
mean_abun <- tapply(tmp, config, mean, na.rm=T)
median_abun <-  tapply(tmp, config,  median, na.rm=T)
z_score <- statistic(wilcoxsign_test(g1~g2))
out[i, 1:9] <- c(wilcox_sign, or, mean_abun, median_abun, z_score, effect)
}
# out
rownames(out) <- colnames(data)
colnames(out) <- c("sign_p.value",paste0(rep(nlevels,3),
c("_ratio", "_mean", "_median")), "z_score", "effect_size")
out <- as.data.frame(out)
out$p.adjust <- p.adjust(out$sign_p.value, method = "BH")
out$enrich <- ifelse(out$p.adjust<0.05, ifelse(out$z_score>0,
nlevels[1], nlevels[2]), "none")
return(out)
}
??dudi.pco
?pam
??pam
rep(c("a","b"), 2)
paste0(rep(c("a","b"), 2), c("ratio", "mean"))
paste0(rep(c("a","b"), 2), rep(c("ratio", "mean"), 2))
paste0(rep(c("a","b"), 2), rep(c("ratio", "mean"), each =2))
mywilcox_2t <- function(datamatrix, configdata, Time2, ratio="zero",...){
# to sort the data
config <- matchID(configdata, ...)
data <- datamatrix[rownames(config), ,drop=F]
# to analysis
out <- matrix(NA, nrow = ncol(data), ncol = 9)
config <- as.factor(as.character(config[, Time2]))
nlevels = levels(droplevels(config))
for(i in 1:ncol(data)){
tmp <- as.numeric(data[,i])
g1 <- tmp[config == nlevels[1]]
g2 <- tmp[config == nlevels[2]]
wilcox_sign <- pvalue(wilcoxsign_test(g1~g2))
effect <- wilcoxonPairedR(x <- tmp, g <- config)
if(ratio=="zero"){
or <- tapply(tmp, config, function(x){sum(x!=0, na.rm=T)/length(x)})
}else{
or <- tapply(tmp, config, function(x){sum(!is.na(x))/length(x)})
}
mean_abun <- tapply(tmp, config, mean, na.rm=T)
median_abun <-  tapply(tmp, config,  median, na.rm=T)
z_score <- statistic(wilcoxsign_test(g1~g2))
out[i, 1:9] <- c(wilcox_sign, or, mean_abun, median_abun, z_score, effect)
}
# out
rownames(out) <- colnames(data)
colnames(out) <- c("sign_p.value",paste0(rep(nlevels,3),
rep(c("_ratio", "_mean", "_median"), each=2)),
"z_score", "effect_size")
out <- as.data.frame(out)
out$p.adjust <- p.adjust(out$sign_p.value, method = "BH")
out$enrich <- ifelse(out$p.adjust<0.05, ifelse(out$z_score>0,
nlevels[1], nlevels[2]), "none")
return(out)
}
library(caret)
?createFolds
createFolds(c(1:100), k = 10)
createFolds(c(2:100), k = 10)
createFolds(c(2:200), k = 10)
unlist(createFolds(c(2:200), k = 10))
as.vector(unlist(createFolds(c(2:200), k = 10)))
r2Twopartmodelcv <- function(dat , phe, response, cutoff, number=10, cutoffp=0.01, repeatN=100, fold,confounder=NULL){
# match the sample ID
id <- intersect(rownames(dat), rownames(phe))
if(length(id)==0){
stop("can't match the sample id")
}
dat <- dat[id, ]
phe <- phe[id, ]
R2 <- c()
for(m in 1:repeatN){
# print(m)
# cross vlidation
sampNum <- nrow(dat)
foldlist <- createFolds(1:sampNum, k = fold)
risk <- rep(NA, sampNum)
for(n in 1:fold){
discSampindex <- c(1:sampNum)[-foldlist[[n]]]
valiSampindex <- foldlist[[n]]
discDatax <- dat[discSampindex, ]
discPhe <- phe[discSampindex, ]
valiDatax <- dat[valiSampindex, ]
valiPhe <- phe[valiSampindex, response]
# twopart Model to get the effect size
twopartRes <- twopartModel(dat = discDatax, phe = discPhe, response = response,
cutoff = cutoff, number = number)
featurelist <- rownames(twopartRes)[twopartRes[,15] <= cutoffp]
if(length(featurelist)==0){
risk[foldlist[[n]]] <- 0  # if no feature, the rish set zero
next
print(paste0("pvalue cutoff :", cutoffp, " can't get any feature"))
}else{
print(paste0("pvalue cutoff :", cutoffp, " get the significant feature ",
length(featurelist)))
}
# get the additive modele
twopartRes2 <- twopartRes[featurelist, ,drop=F]
valiDatax2  <- valiDatax[, featurelist, drop=F]
risk <- c()
for(i in 1:nrow(valiDatax2)){
riskvalue <- c()
for(j in 1:ncol(valiDatax2)){
beta1 <- twopartRes2[, 4]
beta2 <- twopartRes2[, 8]
b <- ifelse(valiDatax2[i, j] <= cutoff, 0, 1)
q <- ifelse(valiDatax2[i, j] <= cutoff, 0, log10(valiDatax2[i,j]))
riskvalue[j] <- beta1+b+beta2*q  # can't not ecsure why add b
}
risk[foldlist[[n]][i]] <- sum(riskvalue)
}
}
# get the R square
foldindex <- as.vector(unlist(foldlist))
if(!is.null(confounder)){
# risk
tmp <- phe[foldindex, c(response, confounder)]
tmp$risk <- risk
formula <- as.formula(paste0(response, "~."))
lmmode <- summary(lm(formula, data = tmp))
# no risk
tmp2 <- tmp[,-ncol(tmp)]
formula2 <- as.formula(paste0(response, "~."))
lmmode2 <- summary(lm(formula2, data = tmp2))
#ã€€from the rsq.partial in the rsq package
R2[m] <- 1-((1-lmmode$r.squared)/(1-lmmode2$r.squared))*(lmmode2$df[2]/lmmode$df[2])
#R2[m] <- lmmode$adj.r.squared-lmmode2$adj.r.squared
}else{
y <- phe[foldindex, response]
print(y)
print(risk)
lmmode <- summary(lm(y~risk))
R2[m] <- lmmode$r.squared
}
}
return(R2)
}
createFolds(c(1:100), k = 10) -> foldlist
foldlist[[1]]
foldlist[[2]]
foldlist[[3]]
?aes_string
library(ggplot)
install.packages("ggplot2")
a <- "f->f"
a
`a`
``a``
tmp <- data.frame("f->f"=c(1:3))
tmp
tmp <- data.frame("f:f"=c(1:3))
tmp
tmp <- data.frame("3f"=c(1:3))
tmp
install.packages("Rd2roxygen")
library(Rtools)
library(Rtools)
install.packages("Rtools")
install.packages("clusterSim")
install.packages("ggpubr")
install.packages("igraph")
install.packages("glmnet")
install.packages("metfor")
install.packages("metafor")
install.packages("metafor")
install.packages("metafor")
library(rmeta)
library(reshape)
library(pheatmap)
library(glmnet)
# generate the result
out <- data.frame(True = 1, Predict = 1, type1 = "", type2 = "")
out2 <- data.frame(Type = "", Score = 1, type1 = "", type2 = "")
Rresult <- matrix(NA, nrow=length(responsename), ncol=length(datalist))
colnames(Rresult) <-  dataname
rownames(Rresult) <- responsename
feature_plot <- out2[-1, ]
recast(feature_plot, Type+type2~type1) -> qdat
annotation_row <- as.data.frame(qdat[, "type2", drop = F])
rownames(annotation_row) <- paste0("test", 1:nrow(annotation_row))
qdat2 <- qdat[, 3:10]
rownames(qdat2) <- rownames(annotation_row)
qdat2[is.na(qdat2)] <- 0
qdat2[qdat2<0] <- -1
qdat2[qdat2>0] <- 1
# remove the only once
index <- apply(abs(qdat2), 1, function(x){sum(x)>=2})
annotation_row <- annotation_row[index,, drop=F]
qdat3 <- as.data.frame(qdat2[index, ])
head(feature_plot)
head(qdat)
head(qdat2)
qdat3
?recast
recast
?cv.glmnet
?createFolds
library(caret)
?createFolds
createFolds(c(1:100),5)
createFolds(c(1:100),5)
createFolds(c(100:200),5)
?cv.glmnet
set.seed(1010)
n = 1000
p = 100
nzc = trunc(p/10)
x = matrix(rnorm(n * p), n, p)
beta = rnorm(nzc)
fx = x[, seq(nzc)] %*% beta
eps = rnorm(n) * 5
y = drop(fx + eps)
px = exp(fx)
px = px/(1 + px)
ly = rbinom(n = length(px), prob = px, size = 1)
set.seed(1011)
cvob1 = cv.glmnet(x, y)
set.seed(NULL)
cvob1 = cv.glmnet(x, y)
cvob1$lambda.1se
?plot
kwmeta <- function(pr, config){
# set the alpha
alpha <- 0.05  # 0.05
# match names of files
inter <- intersect(rownames(config), rownames(pr))
pr <- t(pr[inter, ])
config <-config[inter,,drop=F]
sum_name <- sum(ifelse(rownames(config)==colnames(pr),1,0))
if(sum_name==nrow(config)){print ("All sample matched")}
print(paste0("the sample number is ",length(inter)))
num <- nrow(pr)
fr <- as.factor(config[, 1])
group <- levels(fr)
print(group)
#output
len <- length(group)
num2 <- len*len/2+3.5*len + 2
out <- matrix(NA, num, num2)
ktp <- apply(pr, 1, function(x){kruskal.test(x ~ fr)$p.value})
#post hoc dunn test
library(PMCMR)
for (i in 1:num) {
pr1 <- as.numeric(pr[i,])
index <- is.na(pr1)
pr1 <- pr1[!index]
fr1 <- fr[!index]
rk  <- rank(pr1)
res <- c(ktp[i], tapply(pr1, fr1, median),tapply(pr1,fr1,mean), tapply(pr1>0, fr1,mean))
dtp <- posthoc.kruskal.dunn.test(pr1, fr1, p.adjust.method = "BH")$p.value
dtp <- cbind(dtp, rep(NA, len - 1))
dtp <- rbind(rep(NA, len), dtp)
dtp[upper.tri(dtp)] <- t(dtp)[upper.tri(dtp)]
rownames(dtp)[1] <- colnames(dtp)[1]
colnames(dtp)[len] <- rownames(dtp)[len]
mean_rank <- tapply(rank(pr1),fr1,mean)
res <- c(res,dtp[lower.tri(dtp)], mean_rank)
#
conclude <- rep(0,2*len-1)
or <- order(mean_rank)
conclude[2*(1:len)-1] <- group[or]
op <- rep(1,len-1)
for(j in 1:(len-1)){op[j] <- dtp[or[j],or[j+1]]}
symbol <- rep("=",len-1)
symbol[!is.na(op) & op <= alpha] <- "<"
symbol[is.na(op) | op == 1] <- "<=>"
for(x in 1:(len-1)){
if(symbol[x]=="<"){
p_tmp <- c()
for(y in 1:x){
for(z in (x+1):len){
p_tmp <- c(p_tmp,dtp[or[y],or[z]])
}
}
if(any(p_tmp>0.05)){symbol[x] <- "="}
}
}
conclude[(1:(len - 1)) * 2] <- symbol
res <- c(res, paste(conclude, collapse = " "))
if(length(res)==ncol(out)){out[i, ] <- res}else{print (res)}
}
rownames(out) <- rownames(pr)
cn <- c("kw.p", paste("median", group[1:len], sep = "_"))
cn <- c(cn,paste("mean", group[1:len], sep = "_"))
cn <- c(cn, paste("or", group[1:len], sep = "_"))
cn <- c(cn, paste0("p", "_", group[row(dtp)[lower.tri(dtp)]], "_", group[col(dtp)[lower.tri(dtp)]]))
cn <- c(cn, paste("mean_rank",group[1:len], sep = "_"))
cn <- c(cn, "nearby")
colnames(out) <- cn
return(out)
}
log10c(1:10)
log10(c(1:10))
?loess
loess(dist ~ speed, cars)
predict(loess(dist ~ speed, cars))
cars$dist
predict(loess(dist ~ speed, cars, span = 0.3))
predict(loess(dist ~ speed, cars, span = 0.5))
?cluster
library(cluster)
?hclust
hc <- hclust(dist(USArrests)^2, "cen")
hc
hc$labels
hc$merge
plot(hc)
library(NbClust)
install.packages("NbClust")
library(NbClust)
?NbClust
x<-rbind(matrix(rnorm(100,sd=0.1),ncol=2),
matrix(rnorm(100,mean=1,sd=0.2),ncol=2),
matrix(rnorm(100,mean=5,sd=0.1),ncol=2),
matrix(rnorm(100,mean=7,sd=0.2),ncol=2))
res<-NbClust(x, distance = "euclidean", min.nc=2, max.nc=8,
method = "complete", index = "ch")
x
res
res$Best.nc
res$Best.nc[1]
cuttree(res)
cutree(res)
example(cutree)
ggplot(data = df, mapping = aes(x = var, y = value, group=variable))
+ geom_line() + geom_point()
library(ggplot2)
ggplot(qdat, mapping = aes(x = var, y = value, group=variable))
+ geom_line() + geom_point()
head(qdat)
names(clusplot())
names(clustlable
)
tapply(vector, index, function)
